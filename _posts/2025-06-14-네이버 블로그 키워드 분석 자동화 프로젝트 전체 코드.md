---

title: "[python] 네이버 블로그 키워드 분석 자동화 프로젝트 전체 코드"
date: 2025-06-14 04:07:00 +0900
categories: [data-analysis, naver, sns, keyword]
tags: [네이버, 크롤링, 워드클라우드, python, 자동화, 인사이트,sentiment-analysis, 데이터마케팅, 자동화분석, 데이터리서치]
pin: true

---

# 🐍 네이버 블로그 크롤러

## [colab 환경] 크롬드라이버, 필수 패키지 설치 및 셋업 
```
!apt-get update
!apt install -y chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin/
!pip install selenium beautifulsoup4 pandas
!pip install chromedriver-autoinstaller

import chromedriver_autoinstaller
chromedriver_autoinstaller.install()

import requests
import json
import urllib.parse
import re
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
import pandas as pd
from datetime import datetime
import pytz
from google.colab import files
```

## 1. 네이버 블로그 검색 결과에서 url/제목/날짜 수집 (OpenAPI 사용)
```
def naver_blog_search(client_id, client_secret, keyword, end=1, display=10, sleep_sec=1.0):
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    encText = urllib.parse.quote(keyword)
    urls, dates, titles = [], [], []
    for page in range(end):
        start = page * display + 1
        url = f"https://openapi.naver.com/v1/search/blog?query={encText}&start={start}&display={display}"
        res = requests.get(url, headers=headers)
        if res.status_code == 200:
            data = res.json().get('items', [])
            for row in data:
                link = row.get('link', '')
                if 'blog.naver' in link:
                    urls.append(link)
                    dates.append(row.get('postdate', ''))
                    # 제목 내 HTML 태그 제거
                    title = re.sub('<[^>]*>', '', row.get('title', ''))
                    titles.append(title)
            print(f'페이지 {page+1} 완료 (누적 {len(urls)}건)')
        else:
            print(f"API Error: {res.status_code}")
        time.sleep(sleep_sec)  # API 과부하 방지
    return urls, dates, titles
```

## 2. 셀레니움 웹드라이버 설정 (colab/서버 환경에 맞게 headless)
```
def get_driver():
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1280x1696')
    options.add_argument('--remote-debugging-port=9222')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    chromedriver_path = chromedriver_autoinstaller.install()
    service = Service(chromedriver_path)
    driver = webdriver.Chrome(service=service, options=options)
    driver.implicitly_wait(3)  # 암시적 대기 (로딩 안정화)
    return driver
```

## 3. 네이버 블로그 본문 크롤링 (iframe 자동 감지/진입)
```
def extract_content(driver, url, max_wait=5):
    try:
        driver.get(url)
        # mainFrame iframe 로드될 때까지 대기 후 진입
        for _ in range(max_wait * 2):
            try:
                iframe = driver.find_element(By.ID, "mainFrame")
                driver.switch_to.frame(iframe)
                break
            except:
                time.sleep(0.5)
        html = BeautifulSoup(driver.page_source, "html.parser")
        # 에디터 유형별로 본문 div 탐색
        main = html.select_one("div.se-main-container") or html.select_one("#postViewArea")
        if main:
            text = main.get_text(separator="\n", strip=True)
            return text.strip()
        return ""
    except Exception as e:
        print(f"[본문 파싱 에러] {url}: {e}")
        return ""
```

## 4. 전체 크롤링 파이프라인 (검색 → 본문 크롤 → CSV 저장/다운로드)
```
def naver_blog_crawler(client_id, client_secret, keyword, end=1, display=10, sleep_sec=1.2, prefix='blog'):
    # 파일명: blog_날짜_시간.csv 형태 자동생성 (KST)
    KST = pytz.timezone('Asia/Seoul')
    now = datetime.now(KST).strftime('%Y%m%d_%H%M%S')
    out_csv = f'blog_{now}.csv'

    # 1단계: 검색결과 url/날짜/제목 리스트 수집
    urls, dates, titles = naver_blog_search(client_id, client_secret, keyword, end, display, sleep_sec)
    driver = get_driver()
    contents = []
    # 2단계: 각 블로그 url에서 본문 크롤링
    for idx, url in enumerate(urls):
        print(f"{idx+1}/{len(urls)} 본문 크롤링: {url}")
        contents.append(extract_content(driver, url))
    driver.quit()
    # 3단계: DataFrame으로 저장 후 CSV로 내보내기
    df = pd.DataFrame({'title': titles, 'content': contents, 'date': dates})
    df.to_csv(out_csv, index=False, encoding='utf-8-sig')
    print(f"\n{out_csv}로 저장 완료! (총 {len(df)}개)")
    files.download(out_csv)  # colab 파일 다운로드
```

## 5. 실행부: 사용자 입력 받아 크롤링 시작
```
client_id = 'bumtntsqPoT7Gz1aqUhW'
client_secret = 'beSV3TZU3p'
keyword = input("검색할 키워드: ").strip()
end = input("가져올 페이지 수 (기본 1): ").strip()
display = input("한 페이지당 개수 (기본 10): ").strip()
end = int(end) if end.isdigit() else 1
display = int(display) if display.isdigit() else 10
naver_blog_crawler(client_id, client_secret, keyword, end, display, sleep_sec=1.5, prefix='blog')
```
---
# ☁️ csv전처리 + 워드클라우드 시각화

## 1. 한글 자연어/워드클라우드 분석 패키지 설치 

```
!pip install konlpy
!apt-get -y install fonts-nanum
```

## 2. 필수 라이브러리 임포트
```
from konlpy.tag import Okt
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import re
import numpy as np
from PIL import Image
import os
```

## + 나눔폰트 경로 지정 (워드클라우드 한글출력용)
```
font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
```
## + 마스크 이미지 업로드 및 변환
```
from google.colab import files

def get_mask_image():
    print('마스크 이미지를 업로드 하시겠습니까? (y/N)')
    if input().strip().lower() == 'y':
        up = files.upload()
        mask_fname = list(up.keys())[0]
        try:
            mask = np.array(Image.open(mask_fname))
            print(f"[INFO] 마스크 이미지 적용: {mask_fname}")
            return mask
        except Exception as e:
            print("[경고] 이미지 열기 실패. 기본 사각형 모드로 실행합니다.")
            return None
    else:
        print("[INFO] 기본(네모) 워드클라우드로 생성합니다.")
        return None

mask = get_mask_image()  # 마스크 이미지 적용(선택적)
```

## 3. 분석할 CSV 파일 업로드
```
print("\n분석할 CSV 파일을 업로드하세요")
up = files.upload()
filename = list(up.keys())[0]
```

## 4. 데이터프레임 불러오기 및 본문 전체 합치기
```
df = pd.read_csv(filename)
text = ' '.join(df['content'].astype(str))
```

## 5. 특수문자/기호/영어 등 제거 (한글, 숫자, 공백만 남김)
```
text = re.sub(r'[^\w\sㄱ-ㅎ가-힣]', ' ', text)
text = text.replace('\n', ' ')
```

## 6. Okt 형태소분석기로 명사만 추출
```
okt = Okt()
nouns = okt.nouns(text)
print("명사 샘플:", nouns[:20])
```

## 7. 불용어 제거

```
stopwords = set([
    '것','더','또','에서','하는','하면','하고','요','자','로','와','과','도','을','를','은','는','이','가','에','의','한','및','고','나','보다','밖에','처럼','부터',
    '내가','내','너무','정말','매우','진짜','같아요','ㅎㅎ','저는','있는','있어서','있는','있고','있다'
])
words = [w for w in nouns if w not in stopwords and len(w) > 1]
```

## 8. 단어별 등장 빈도 계산
```
word_count = Counter(words)
```

## 9. 워드클라우드 생성 (마스크 이미지 선택)
```
wc = WordCloud(
    font_path=font_path, 
    background_color='white',
    width=800, height=800,
    mask=mask   # None이면 네모, 선택 가능
)
wc_img = wc.generate_from_frequencies(word_count)
```

## 10. 워드클라우드 시각화
```
plt.figure(figsize=(10, 10))
plt.imshow(wc_img, interpolation='bilinear')
plt.axis('off')
plt.show()
```

## 11. 등장 빈도 Top 30 명사 출력
```
print('\n상위 30개 명사:')
for word, count in word_count.most_common(30):
    print(f"{word}: {count}")
```
